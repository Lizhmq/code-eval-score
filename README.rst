# Code-eval-score
This is a package for evaluting the quality of code generated by AI models.

We provide two ways to evaluate the code quality:
* MatchScore: uses the CodeBERT model to calculate the similarity between the hypothesis code and the reference code.
* GenScore: uses the CodeT5 model to calculate the probability of generating hypothesis code from the reference code.

Paper comming soon.
[Contact to the author](https://github.com/Lizhmq)